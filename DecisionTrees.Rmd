---
title: "Decision Trees"
output: pdf_document
---

This script will go over Decision Trees in R. It will not cover generalization such as SEM Trees or Rasch Trees\
\
Resources:\
Basic intro to Decision Trees: http://www.statmethods.net/advstats/cart.html\
Full list of data mining packages in R: http://cran.r-project.org/web/views/MachineLearning.html\
\
For longitudinal data:\
REEMtree\
\
Two packages will be used and their caret equivalents:\
**rpart** (tree accomplishes very similar thing):http://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf\
\
**party**: http://cran.r-project.org/web/packages/party/vignettes/party.pdf\
\
In caret, method = \
"rpart" -- tuning = cp (complexity parameter)\
"rpart2" -- tuning = maxdepth\
"rpartCost" -- tuning = cp and cost\
"ctree" -- tuning = mincriterion (p value thresholds)\
"ctree2" -- tuning = maxdepth\
(see "train_model_list" in caret reference manual)\
\
Bonus:\
non-greedy tree algorithm:\
**evtree**: http://cran.r-project.org/web/packages/evtree/vignettes/evtree.pdf\
\
Lets load the main packages
```{r,warning=F,message=FALSE}
library(caret)
library(rpart)
library(pROC)
library(party)
library(MASS) # for boston data
data(Boston)
```


# Regression (continous outcome)#

Use rpart first with the Boston data
use regression first -- predicting median value of homes

```{r}
#str(Boston)

# lets get a baseline with linear regression
lm.Boston <- lm(medv ~., data=Boston)
#summary(lm.Boston)
```

We do pretty well with linear regression
R-squared of .74

## CART

How about if we just blindly apply Decision Trees

```{r,fig.height=6}
rpart.Boston <- rpart(medv ~., data=Boston)
#summary(rpart.Boston)
plot(rpart.Boston);text(rpart.Boston)

# this can be hard to interpret, so I like to look at a different output
rpart.Boston

pred1 <- predict(rpart.Boston)
cor(pred1,Boston$medv)**2
```
Doing really well -- Rsquared = 0.81\

## Lasso Regression

What if we tried regularized (penalized) regression instead?\
Note: for glmnet, both the x's and y have to be in separate matrices\
-- and all class = numeric\
-- don't worry about response, doesn't have to be factor for logistic\
----- just specify "binomial"\

```{r,message=FALSE}
y.B <- Boston$medv
x.B <- sapply(Boston[,-14],as.numeric)

# alpha =1 for lasso, 0 for ridge
library(glmnet)
cv <- cv.glmnet(x.B,y.B,alpha=1)
lasso.reg <- glmnet(x.B,y.B,alpha=1,family="gaussian",lambda=cv$lambda.min)

lasso.resp <- predict(lasso.reg,newx=x.B)
cor(y.B,lasso.resp)**2

```

Taking into account cross-validation, we do worse compared to linear regression with no tuning.\


So the plot for rpart didn'tcome out that well.\
Good news, there are better options for plotting.\
\
http://blog.revolutionanalytics.com/2013/06/plotting-classification-and-regression-trees-with-plotrpart.html
\
Let's load some new packages:
```{r}
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(partykit)
```

Note: rattle is package that uses a GUI (think SPSS) for data mining applications
check out book: http://www.amazon.com/Data-Mining-Rattle-Excavating-Knowledge/dp/1441998896

Anyways, lets try some new, prettier plots:

```{r}
# prp(); from rpart.plot
prp(rpart.Boston);text(rpart.Boston)

```
Note, prp() offers many additional capabilities for tweaking the plot
For instance:
```{r}
# ?prp
prp(rpart.Boston,varlen=10,digits=5,fallen.leaves=T)
```

```{r}
#fancyRpartPlot(); from rattle
fancyRpartPlot(rpart.Boston)
```

## Conditional Inference Trees
So what about with conditional inference trees?

What if we want a smaller tree? This can be accomplished a number of ways. We can prespecify the maxdepth, the minimum number of people per node, as well as making more restrictive splitting criterion.

Example of prespecifying the depth with ctree()
```{r}
ctree.Boston <- ctree(medv ~., data=Boston)
#plot(ctree.Boston) # too big of a tree
pred2 <- predict(ctree.Boston)
cor(pred2,Boston$medv)**2
```
We do better than rpart, Rsquared = 0.87\
\
Biggest difference between ctree() and rpart() is that ctree() does not demonstrate bias with respect to the number of response options, and supposedly had less of a propensity to overfit than rpart().
\
Note: the models are not optimizing based on Rsquared, most likely MSE\
\
So what do we think now? Are we happy with results?
Remember, decision trees are generally quite robust, so it may not be necessary to check assumptions. -- See Table 10.1 ESL\
\
But what about generalizability?\
\
Although not as serious as with SVM for instance, Decision Trees have a propensity to overfit, meaning the tree structure won't generalize well\
\
So let's try just creating a simple Training and Test datasets

```{r}
train = sample(dim(Boston)[1], dim(Boston)[1]/2) # half of sample
Boston.train = Boston[train, ]
Boston.test = Boston[-train, ]
```


Try linear regression first
```{r}
lm.train <- lm(medv ~., data=Boston.train)

pred.lmTest <- predict(lm.train,Boston.test)
cor(pred.lmTest,Boston.test$medv)**2
```
Note: we are taking our lm object trained on the train dataset, and using these fixed coefficients to predict values on the test dataset.\
\
In SEM, this is referred to as a tight replication strategy
No difference in using a test dataset -- both Rsq are 0.74\
\
How about with rpart?

```{r}
rpart.train <- rpart(medv ~., data=Boston.train)

pred.rpartTest <- predict(rpart.train,Boston.test)
cor(pred.rpartTest,Boston.test$medv)**2

```

Not as good -- drops from 0.81 to 0.76 -- still better than lm()\

But with rpart, it is common to prune trees back. What if we try this, is there less of a drop in $R^{2}$?

Note: rpart automatically does internal CV, varying the complexity paramter (cp). If you use the tree package instead, you will have to use cv.tree()

With plotcp() we are going to choose the error within 1 SE of the lowest cross-validated error. This will be used to prune

```{r}
plotcp(rpart.train)
printcp(rpart.train)
prune.Bos <- prune(rpart.train,0.017)

#plot(prune.Bos);text(prune.Bos)
fancyRpartPlot(prune.Bos)
```

caret:
```{r}
ctree.train <- ctree(medv ~., data=Boston.train)

pred.ctreeTest <- predict(ctree.train,Boston.test)
cor(pred.ctreeTest,Boston.test$medv)**2

```
Drops from 0.87 to 0.77\
\
Better than rpart (barely) and lm()\
\
It is worth noting how much more of an effect there was for using a test dataset with the tree methods as compared to lm(), this is pretty typical, and much more important with more "flexible" methods such as random forests, gbm, svm etc...


# Classification (categorical outcome)#



**Two Biggest Things To Remember:**\
1. Make sure functions outcome variable is categorical; as.factor(outcome)\
2. Using predict() changes. Variable across packages\
\

As a baseline, we will use logistic regression.
```{r}
library(ISLR)
data(Default)
head(Default)
str(Default)
```
My favorite function in R is str(), as it gives the class of each variable and other summary characteristics. Most important thing to note is that the "default" variable is already coded as a factor variable, meaning that R now knows it is categorical, and will change the cost function (thus estimator) accordingly.\
\
This is really important because rpart,randomForest and other packages do not automatically detect whether it is a regression or classification problem. If you don't change the outcome variable to its proper class, you could get a suboptimal answer (use the wrong estimator i.e. regression instead of logistic regression)\
\ 
Now let's do logistic regression
```{r}
lr.out <- glm(default~student+balance+income,family="binomial",data=Default)
summary(lr.out)
```

I always find it much harder to figure out how well I am doing with logistic regression. One of the best ways to assess results in my opinion is the use of receiver operating characteristic curves (ROC curves).\
\
These plots are a balanace of sensitivity and specificity. Ideally the curve gets as close as possible to the upper left corner.\
\
To get this plot, we need to get our predictions from our logistic model.

```{r}
glm.probs=predict(lr.out,type="response")
#glm.pred00=ifelse(glm.probs>0.5,1,0)

rocCurve <- roc(Default$default,glm.probs)
pROC::auc(rocCurve)
pROC::ci.roc(rocCurve)

# quartz()
plot(rocCurve, legacy.axes = TRUE,print.thres=T,print.auc=T)
```

For AUC (area under the curve), values of 0.8 and 0.9 are good (the higher the better)

### predict() with missing variables ###

How about lasso logistic regression?
```{r}
library(glmnet)
yy = as.numeric(Default$default)
xx = sapply(Default[,2:4],as.numeric)
lasso.out <- cv.glmnet(xx,yy,family="binomial",alpha=1,nfolds=10) #alpha=1 ==  lasso; 0 = 
# find best lambda
ll <- lasso.out$lambda.min

lasso.probs <- predict(lasso.out,newx=xx,s=ll,type="response")
```

Results from lasso using CV
```{r}

rocCurve.lasso <- roc(Default$default,lasso.probs)
pROC::auc(rocCurve.lasso)
pROC::ci.roc(rocCurve.lasso)

# quartz()
plot(rocCurve.lasso, legacy.axes = TRUE,print.thres=T,print.auc=T)
```

Almost identical results to logistic regression with no penalization.

## Using Decision Trees for Classification

Instead of demonstrating how to use rpart() or ctree(), I prefer to use the train() from caret. This makes it much easier to test out multiple different methods, as well as automatically vary the tuning parameters such as depth, complexity etc..

train() for ctree
```{r}
train.ctree <- train(as.factor(default)~student+balance+income,data=Default,method="ctree")
plot(train.ctree)
```

train() for rpart
```{r}
train.rpart <- train(as.factor(default)~student+balance+income,data=Default,method="rpart")
plot(train.rpart)
```

In train() and through trainControl() you can see that it automatically varies different tuning parameters (see caret documentation for the different options for each method), while defaulting to bootstrap estimation to test out each. This is a great way to prevent overfitting.

In examining both plots, it seems as both methods do comparably well, while also they both have different tuning parameters (X-axis). Based on these plots, I would increase the number of values for the tuning parameters, as the accuracy did not reach a maximum necessarily outside of the tails. (tuneLength = 3 is default)

Finally, what if we try a more advanced, flexible tree algorith, Random Forests?

```{r}
Default$r1 <- rnorm(10000)
Default$r2 <- rnorm(10000)
train.rf <- train(as.factor(default) ~.,data=Default,
                  method="rf",tuneLength=4)
plot(train.rf)

train.rf$finalModel$importance
```

You can see the propensity for random forests to overfit by the inclusion and higher importance for two random noise variables added to the dataset